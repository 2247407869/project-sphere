# ä¸»ç¨‹åºå…¥å£ï¼šè´Ÿè´£ Web åº”ç”¨çš„å¯åŠ¨ä¸ API è·¯ç”±è°ƒåº¦ (Debug Mode V2)
from fastapi import FastAPI
from src.utils.config import settings
from src.agents.knowledge_agent import knowledge_graph
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import logging
import json
import os
import time
from datetime import datetime
from src.agents.knowledge_agent import llm
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

# åˆå§‹åŒ–é…ç½®ä¸æ—¥å¿—ç³»ç»Ÿ
import sys
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("Sphere-Core")
logger.propagate = True # ç¡®ä¿æ—¥å¿—å¯ä»¥ä¸Šä¼ 
from pydantic import BaseModel

app = FastAPI(title=settings.PROJECT_NAME)

# å®šä¹‰ API è¯·æ±‚æ¨¡å‹
class CollectRequest(BaseModel):
    content: str
    source: str = "mobile"

class ChatRequest(BaseModel):
    message: str
    history: list = []
    summary: str = "" # æ–°å¢ï¼šå½“å‰å¯¹è¯çš„æ»šåŠ¨æ‘˜è¦

# é…ç½® CORS è·¨åŸŸæ”¯æŒ (å…è®¸ç§»åŠ¨ç«¯ Web è®¿é—®)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£ï¼šç”¨äºéªŒè¯æœåŠ¡æ˜¯å¦åœ¨çº¿"""
    return {"status": "healthy", "project": settings.PROJECT_NAME}

@app.post("/collect")
async def collect_knowledge(req: CollectRequest):
    """
    çŸ¥è¯†é‡‡é›†æ ¸å¿ƒç«¯ç‚¹ï¼šåŒæ­¥ç­‰å¾… AI åˆ†æç»“æœå¹¶è¿”å›
    """
    logger.info(f"æ”¶åˆ°æ¥è‡ª {req.source} çš„é‡‡é›†è¯·æ±‚")
    initial_state = {
        "content": req.content,
        "metadata": {"source": req.source},
        "summary": "",
        "status": "received"
    }
    result = knowledge_graph.invoke(initial_state)
    return {
        "status": result["status"],
        "summary": result["summary"],
        "metadata": result["metadata"]
    }

@app.get("/facts")
async def get_facts():
    """è¯»å–äº‘ç«¯äº‹å®å½’æ¡£ (L3 è®°å¿†)"""
    import json
    import os
    facts_file = os.path.join("data", "facts.json")
    if os.path.exists(facts_file):
        try:
            with open(facts_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            return {"error": f"Failed to read facts: {e}"}
    return []

class SessionSyncRequest(BaseModel):
    history: list
    summary: str

@app.get("/session/load")
async def load_session():
    """ä»äº‘ç«¯æ¢å¤ä¼šè¯çŠ¶æ€"""
    import json
    import os
    session_file = os.path.join("data", "sessions.json")
    if os.path.exists(session_file):
        try:
            with open(session_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {"history": [], "summary": ""}

@app.post("/session/sync")
async def sync_session(req: SessionSyncRequest):
    """åŒæ­¥ä¼šè¯çŠ¶æ€è‡³äº‘ç«¯"""
    import json
    import os
    session_file = os.path.join("data", "sessions.json")
    os.makedirs("data", exist_ok=True)
    try:
        with open(session_file, "w", encoding="utf-8") as f:
            json.dump({"history": req.history, "summary": req.summary}, f, ensure_ascii=False, indent=2)
        return {"status": "synced"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

class TodoItem(BaseModel):
    id: str
    task: str
    completed: bool = False
    created_at: str

@app.get("/todos")
async def get_todos():
    """è·å–æ‰€æœ‰å¾…åŠäº‹é¡¹"""
    import json
    import os
    todo_file = os.path.join("data", "todos.json")
    if os.path.exists(todo_file):
        try:
            with open(todo_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return []

@app.post("/todos/sync")
async def sync_todos(todos: list[TodoItem]):
    """åŒæ­¥å¾…åŠäº‹é¡¹åº“"""
    import json
    import os
    todo_file = os.path.join("data", "todos.json")
    os.makedirs("data", exist_ok=True)
    try:
        with open(todo_file, "w", encoding="utf-8") as f:
            json.dump([todo.dict() for todo in todos], f, ensure_ascii=False, indent=2)
        return {"status": "synced"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/pinned")
async def get_pinned_facts():
    """è·å–æ°¸ä¸å‹ç¼©çš„æ ¸å¿ƒäº‹å® (L2.5)"""
    import json
    import os
    pinned_file = os.path.join("data", "pinned_facts.json")
    if os.path.exists(pinned_file):
        try:
            with open(pinned_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return []

@app.post("/pinned/update")
async def update_pinned_facts(facts: list[str]):
    """æ›´æ–°æ ¸å¿ƒäº‹å®åº“"""
    import json
    import os
    pinned_file = os.path.join("data", "pinned_facts.json")
    os.makedirs("data", exist_ok=True)
    try:
        with open(pinned_file, "w", encoding="utf-8") as f:
            json.dump(facts, f, ensure_ascii=False, indent=2)
        return {"status": "updated"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

from fastapi.staticfiles import StaticFiles
from fastapi.responses import StreamingResponse, FileResponse
import json
import asyncio
import os

# æŒ‚è½½å‰ç«¯é™æ€èµ„æº
# å‡è®¾ frontend ç›®å½•ä¸ main.py åœ¨åŒçº§
frontend_path = os.path.join(os.path.dirname(__file__), "frontend")
if os.path.exists(frontend_path):
    app.mount("/static", StaticFiles(directory=frontend_path), name="static")

@app.get("/")
async def read_index():
    """å…¥å£é‡å®šå‘ï¼šè®¿é—®æ ¹è·¯å¾„æ—¶ç›´æ¥è¿”å›èŠå¤©ç•Œé¢"""
    index_file = os.path.join(frontend_path, "index.html")
    if os.path.exists(index_file):
        return FileResponse(index_file)
    return {"message": "Frontend index.html not found. Please check 'frontend' folder."}

@app.post("/chat")
async def chat_with_agent(req: ChatRequest):
    """
    ä¸‰å±‚è®°å¿†æ¶æ„å¯¹è¯æ¥å£ (TMA Stage 1) - æµå¼ç‰ˆæœ¬:
    1. åŠ¨æ€æ³¨å…¥ L2 æ‘˜è¦ä½œä¸ºâ€œé•¿æœŸèƒŒæ™¯â€
    2. ä½¿ç”¨ StreamingResponse å®ç°æ‰“å­—æœºæ•ˆæœ
    3. åœ¨æµç»“æŸæ—¶å›ä¼  metadata (summary & history)
    """
    import sys, time
    from datetime import datetime
    start_time = time.time()
    # å¼ºåˆ¶åœ¨å‡½æ•°å…¥å£æ‰“æ¡©ï¼Œä¸ä¾èµ– generator å¼€å§‹æ‰§è¡Œ
    entry_msg = f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ [BACKEND HIT] /chat endpoint reached.\n"
    sys.stderr.write(entry_msg)
    sys.stderr.flush()
    
    logger.info("--- [Stream Chat Session Start] ---")
    
    async def chat_generator():
        import sys
        banner = f"\n{'='*30}\nğŸŸ¢ NEW STREAMING REQUEST AT {datetime.now().strftime('%H:%M:%S.%f')[:-3]}\n{'='*30}\n"
        sys.stderr.write(banner)
        sys.stderr.write(f"RAW USER TEXT: {req.message}\n")
        sys.stderr.flush()
        # --- L2.5 Pinned Context æ³¨å…¥ ---
        pinned_facts = []
        pinned_file = os.path.join("data", "pinned_facts.json")
        if os.path.exists(pinned_file):
            try:
                with open(pinned_file, "r", encoding="utf-8") as f:
                    pinned_facts = json.load(f)
            except: pass
            
        system_content = "ä½ æ˜¯ä¸€ä½åšå­¦ä¸”ä¸¥è°¨çš„æŠ€æœ¯åŠ©æ‰‹ã€‚ä½ çš„ç›®æ ‡æ˜¯ååŠ©ç”¨æˆ·æ„å»ºçŸ¥è¯†åº“ã€‚"
        if pinned_facts:
            system_content += "\n\nã€æ ¸å¿ƒé”å®šäº‹å®ï¼ˆæ°¸ä¸å‹ç¼©ï¼‰ã€‘:\n" + "\n".join([f"- {f}" for f in pinned_facts])
            
        if req.summary:
            system_content += f"\n\nã€å‰æƒ…æè¦ï¼ˆåŠ¨æ€è®°å¿†ï¼‰ã€‘ï¼š\n{req.summary}"
            
        # --- æ—¥å¿—è¿½è¸ªï¼šæš´éœ²ç»™ææ—æ¾çœ‹çš„ä¸Šä¸‹æ–‡ ---
        logger.info(f">>> [System Prompt Context]:\n{system_content}")
        logger.info(f">>> [Chat History Window]: {len(req.history)} messages")

        messages = [SystemMessage(content=system_content)]
        for h in req.history:
            if h["role"] == "user":
                messages.append(HumanMessage(content=h["content"]))
            else:
                messages.append(AIMessage(content=h["content"]))
        
        # è¡¥ä¸Šå½“å‰æœ€åä¸€æ¡ç”¨æˆ·çš„æé—®
        messages.append(HumanMessage(content=req.message))

        # --- æ ¸å¿ƒè°ƒè¯•æ—¥å¿—ï¼šåŒæ­¥å†™å…¥ debug_prompt.txt (æœ€é«˜ä¼˜å…ˆçº§å¤‡ä»½) ---
        try:
            debug_info = f"\n{'='*50}\nTIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')}\n"
            for i, m in enumerate(messages):
                role = "SYSTEM" if isinstance(m, SystemMessage) else "USER" if isinstance(m, HumanMessage) else "ASSISTANT"
                debug_info += f"\n[{i}] {role}:\n{m.content}\n"
            debug_info += f"{'='*50}\n"
            with open("debug_prompt.txt", "w", encoding="utf-8") as df:
                df.write(debug_info)
        except Exception as de:
            logger.error(f"Failed to write debug_prompt.txt: {de}")

        sys.stderr.write(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ“ Raw prompt has been synced to 'debug_prompt.txt'\n")
        sys.stderr.flush()
        
        full_content = ""
        try:
            # 1. å¼€å¯ LLM å¼‚æ­¥æµ
            async for chunk in llm.astream(messages):
                token = chunk.content
                full_content += token
                yield token
            
            chat_done_time = time.time()
            logger.info(f"LLM First Response Latency: {chat_done_time - start_time:.2f}s")

            # 2. å¯¹è¯ç»“æŸåï¼Œå¤„ç†è®°å¿†é€»è¾‘ (L2 å‹ç¼©)
            new_summary = req.summary
            new_history = req.history + [{"role": "user", "content": req.message}, {"role": "ai", "content": full_content}]
            
            # è§¦å‘æ¡ä»¶ï¼šå†å²è¿‡é•¿(>12) OR (å†å²ç§¯ç´¯åˆ°ä¸€å®šç¨‹åº¦(>3) ä¸” æ‘˜è¦å°šä¸ºç©º)
            should_compress = len(new_history) >= 12
            is_initial_summary = (len(new_history) >= 6 and not req.summary) # 6æ¡æ¶ˆæ¯å³3è½®å¯¹è¯
            
            if should_compress or is_initial_summary: 
                logger.info(f"!! [TMA Triggered] Reason: {'Long Context' if should_compress else 'Initial Summary'}")
                compress_prompt = f"""
                åŸºäºä»¥ä¸‹ã€å‰æƒ…æè¦ã€‘å’Œã€æ–°å¢å¯¹è¯ã€‘ï¼Œç”Ÿæˆä¸€ä¸ªå†…å®¹ä¸°æ»¡ä¸”ç»“æ„åŒ–çš„æ–°æ‘˜è¦ï¼ˆ300å­—ä»¥å†…ï¼‰ã€‚
                è¦æ±‚ï¼š
                1. é‡‡ç”¨æ— åºåˆ—è¡¨å½¢å¼ã€‚
                2. å¿…é¡»ä¿ç•™å…³é”®çš„æŠ€æœ¯å‚æ•°ã€æ ¸å¿ƒæ¶æ„å†³ç­–ã€ç”¨æˆ·æ˜¾å¼æåŠçš„é‡è¦åæ­¥ã€‚
                3. ã€é‡è¦ã€‘ä¸å†éœ€è¦åŒ…å«å¾…åŠäº‹é¡¹ï¼Œå¾…åŠäº‹é¡¹å°†ç”±ç‹¬ç«‹æ¨¡å—ç®¡ç†ã€‚
                
                åŸæè¦ï¼š{req.summary}
                æ–°å¢å†…å®¹ï¼š{new_history[-1]["content"]} 
                """
                summary_response = await llm.ainvoke([
                    SystemMessage(content="ä½ æ˜¯ä¸€ä½è®°å¿†ç®¡ç†ä¸“å®¶ã€‚ä½ åªè¾“å‡ºæè‡´å‹ç¼©åçš„äº‹å®æ€»ç»“ï¼Œä¸å«åºŸè¯ã€‚"),
                    HumanMessage(content=compress_prompt)
                ])
                new_summary = summary_response.content
                new_history = new_history[-4:]

            # --- å…¨å±€çŠ¶æ€æ„ŸçŸ¥é€»è¾‘ (ToDo + L2.5 Pinned + L3 Auto Facts) ---
            pinned_file = os.path.join("data", "pinned_facts.json")
            todo_file = os.path.join("data", "todos.json")
            l3_fact_file = os.path.join("data", "facts.json")
            
            current_pinned = []
            if os.path.exists(pinned_file):
                try:
                    with open(pinned_file, "r", encoding="utf-8") as f: current_pinned = json.load(f)
                except: pass
            
            current_todos = []
            if os.path.exists(todo_file):
                try:
                    with open(todo_file, "r", encoding="utf-8") as f: current_todos = json.load(f)
                except: pass

            world_state_prompt = f"""
            ä½œä¸ºç³»ç»Ÿæ¶æ„å¸ˆï¼Œåˆ†æå¯¹è¯å¹¶æ›´æ–°ä»¥ä¸‹ä¸‰ç±»ä¿¡æ¯ï¼š
            
            1. ã€é”å®šäº‹å® (L2.5)ã€‘ï¼šé•¿æœŸä¸å˜çš„é¡¶å±‚æˆ˜ç•¥ã€åŸåˆ™ã€‚ç›®å‰å€¼ï¼š{json.dumps(current_pinned, ensure_ascii=False)}
            2. ã€å¾…åŠäº‹é¡¹ (ToDo)ã€‘ï¼šå…·ä½“çš„çŸ­æœŸåŠ¨ä½œä»»åŠ¡ã€‚å¿…é¡»åŒ…å« id, task, completed, created_atã€‚ç›®å‰å€¼ï¼š{json.dumps(current_todos, ensure_ascii=False)}
            3. ã€æ–°æ²‰æ·€äº‹å® (L3)ã€‘ï¼šæå–æœ¬æ¬¡å¯¹è¯ä¸­äº§ç”Ÿçš„æœ‰ä»·å€¼çš„æ–°äº‹å®ã€æŠ€æœ¯å†³ç­–æˆ–ç”¨æˆ·åå¥½ã€‚åªæå–çœŸæ­£æœ‰æŒä¹…ä»·å€¼çš„ä¿¡æ¯ã€‚
            
            å¯¹è¯ï¼šUser: {req.message} -> AI: {full_content}
            
            è¯·è¿”å›æ›´æ–°åçš„ JSONã€‚å¦‚æœæ˜¯æ–°å¢å¾…åŠï¼Œè¯·æ ¹æ®è¯­ä¹‰è‡ªåŠ¨åˆ›å»ºå¹¶åˆ†é… UUIDã€‚
            {{
                "pinned_facts": ["äº‹å®æè¿°", ...],
                "todos": [
                    {{ "id": "uuid", "task": "ä»»åŠ¡å†…å®¹", "completed": false, "created_at": "ISOæ—¶é—´" }}
                ],
                "new_l3_facts": ["æ–°äº‹å®1", "æ–°äº‹å®2"]
            }}
            åªè¾“å‡º JSONï¼Œä¸å«è§£é‡Šã€‚å¦‚æœæ²¡æœ‰æ–° L3 äº‹å®ï¼Œè¯·è¿”å›ç©ºæ•°ç»„ã€‚
            """
            try:
                state_res = await llm.ainvoke([
                    SystemMessage(content="ä½ æ˜¯ä¸€ä½é«˜æ•ˆçš„çŠ¶æ€ç®¡ç†å™¨ã€‚åªè¾“å‡ºçº¯ JSONã€‚"),
                    HumanMessage(content=world_state_prompt)
                ])
                raw_state = state_res.content.strip()
                if "```json" in raw_state: raw_state = raw_state.split("```json")[1].split("```")[0].strip()
                elif "```" in raw_state: raw_state = raw_state.split("```")[1].split("```")[0].strip()
                
                state_data = json.loads(raw_state)
                # åˆ†å‘æ›´æ–°
                if "pinned_facts" in state_data:
                    current_pinned = state_data["pinned_facts"]
                    with open(pinned_file, "w", encoding="utf-8") as f: json.dump(current_pinned, f, ensure_ascii=False, indent=2)
                
                if "todos" in state_data:
                    current_todos = state_data["todos"]
                    with open(todo_file, "w", encoding="utf-8") as f: json.dump(current_todos, f, ensure_ascii=False, indent=2)
                
                # è‡ªåŠ¨æ²‰æ·€ L3 äº‹å®
                if state_data.get("new_l3_facts"):
                    l3_facts = []
                    if os.path.exists(l3_fact_file):
                        try:
                            with open(l3_fact_file, "r", encoding="utf-8") as f: l3_facts = json.load(f)
                        except: pass
                    
                    for f in state_data["new_l3_facts"]:
                        l3_facts.append({
                            "summary": f,
                            "timestamp": datetime.now().isoformat(),
                            "source": "Auto-Extracted"
                        })
                    with open(l3_fact_file, "w", encoding="utf-8") as f: json.dump(l3_facts, f, ensure_ascii=False, indent=2)

            except Exception as e:
                logger.error(f"World State Refresh Error: {e}")

            # 3. å‘é€å…ƒæ•°æ®æ ‡è®°ä½
            try:
                end_time = time.time()
                metadata = {
                    "type": "metadata",
                    "summary": new_summary,
                    "history": new_history,
                    "pinned_facts": current_pinned,
                    "todos": current_todos,
                    "debug": {
                        "raw_prompt": [
                            {"role": "system" if isinstance(m, SystemMessage) else "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content} 
                            for m in messages
                        ],
                        "latency": {
                            "llm_chat": f"{chat_done_time - start_time:.2f}s",
                            "total": f"{end_time - start_time:.2f}s"
                        },
                        "system_prompt": system_content,
                        "history_count": len(req.history)
                    }
                }
                meta_json = json.dumps(metadata, ensure_ascii=False)
                yield f"\n[METADATA]{meta_json}"
                logger.info(f"--- [Stream Chat End] Total Latency: {end_time - start_time:.2f}s ---")
            except Exception as me:
                logger.error(f"Metadata generation failed: {me}")
                yield f"\n[METADATA]{{\"error\": \"metadata_failed\"}}"

        except Exception as e:
            logger.error(f"Streaming failed: {e}", exc_info=True)
            yield f"Error: å¯¹è¯ä¸­æ–­ï¼Œè¯·é‡è¯•ã€‚"

    return StreamingResponse(chat_generator(), media_type="text/event-stream")

if __name__ == "__main__":
    # å¯åŠ¨ Uvicornï¼Œä¼˜å…ˆè¯»å– HF ç¯å¢ƒè¦æ±‚çš„ç«¯å£
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"Sphere Backend Server is launching on port {port}...")
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)
