# ä¸»ç¨‹åºå…¥å£ï¼šè´Ÿè´£ Web åº”ç”¨çš„å¯åŠ¨ä¸ API è·¯ç”±è°ƒåº¦
import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime
from typing import Optional

import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from pydantic import BaseModel

from src.agents.knowledge_agent import llm
from src.utils.config import settings
from src.utils.scheduler import start_scheduler

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("Sphere-Core")
logger.propagate = True

# å¸¸é‡å®šä¹‰
class Config:
    SESSION_FILE = os.path.join("data", "sessions.json")
    DEBUG_PROMPT_FILE = "debug_prompt.txt"
    DEBUG_STREAM_LOG = "debug_stream.log"
    FRONTEND_PATH = os.path.join(os.path.dirname(__file__), "frontend")
    
    # è¶…æ—¶è®¾ç½®
    CHAT_TIMEOUT = 45.0
    TOOL_TIMEOUT = 30.0
    
    # é™åˆ¶è®¾ç½®
    MAX_TOOLS_PER_ROUND = 5
    MAX_TOOL_ROUNDS = 10
    CONTENT_PREVIEW_LENGTH = 2000

app = FastAPI(title=settings.PROJECT_NAME)

# å®šä¹‰ API è¯·æ±‚æ¨¡å‹
class CollectRequest(BaseModel):
    content: str
    source: str = "mobile"

class ChatRequest(BaseModel):
    message: str
    history: list = []
    summary: str = ""  # å½“å‰å¯¹è¯çš„æ»šåŠ¨æ‘˜è¦
    auto_save: bool = True  # æ˜¯å¦è‡ªåŠ¨ä¿å­˜ä¼šè¯ï¼ˆæµ‹è¯•æ—¶å¯è®¾ä¸º Falseï¼‰

# é…ç½® CORS è·¨åŸŸæ”¯æŒ (å…è®¸ç§»åŠ¨ç«¯ Web è®¿é—®)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    start_scheduler()

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£ï¼šç”¨äºéªŒè¯æœåŠ¡æ˜¯å¦åœ¨çº¿"""
    try:
        # æ£€æŸ¥åŸºæœ¬é…ç½®
        config_status = {
            "deepseek_api_configured": bool(settings.DEEPSEEK_API_KEY),
            "webdav_configured": bool(settings.INFINICLOUD_URL and settings.INFINICLOUD_USER),
            "environment": settings.ENV,
            "debug_mode": settings.DEBUG
        }
        
        # æ£€æŸ¥å­˜å‚¨è¿æ¥ï¼ˆç®€å•æµ‹è¯•ï¼‰
        storage_status = "unknown"
        try:
            from src.storage.sphere_storage import get_sphere_storage
            storage = get_sphere_storage()
            # ç®€å•çš„è¿æ¥æµ‹è¯•
            storage_status = "connected"
        except Exception as e:
            storage_status = f"error: {str(e)[:100]}"
        
        return {
            "status": "healthy",
            "project": settings.PROJECT_NAME,
            "version": "1.0.0",
            "config": config_status,
            "storage": storage_status,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ===== ä¼šè¯ç®¡ç† =====
class SessionSyncRequest(BaseModel):
    history: list
    summary: str

@app.get("/session/load")
async def load_session():
    """ä»äº‘ç«¯æ¢å¤ä¼šè¯çŠ¶æ€"""
    from src.storage.sphere_storage import get_sphere_storage
    storage = get_sphere_storage()
    return await storage.load_current_session()

@app.post("/session/sync")
async def sync_session(req: SessionSyncRequest):
    """åŒæ­¥ä¼šè¯çŠ¶æ€è‡³äº‘ç«¯"""
    from src.storage.sphere_storage import get_sphere_storage
    storage = get_sphere_storage()
    success = await storage.save_current_session(req.history, req.summary)
    if success:
        return {"status": "synced"}
    else:
        return {"status": "error", "message": "åŒæ­¥å¤±è´¥"}

@app.delete("/session/clear")
async def clear_session():
    """æ¸…ç©ºä¼šè¯å†å²å’Œæ‘˜è¦ï¼ˆäº‘ç«¯+æœ¬åœ°ï¼‰"""
    from src.storage.sphere_storage import get_sphere_storage
    storage = get_sphere_storage()
    
    # æ¸…ç©ºäº‘ç«¯
    cloud_success = await storage.clear_current_session()
    
    # æ¸…ç©ºæœ¬åœ°æ–‡ä»¶
    local_success = True
    try:
        if os.path.exists(Config.SESSION_FILE):
            with open(Config.SESSION_FILE, "w", encoding="utf-8") as f:
                json.dump({"history": [], "summary": ""}, f, ensure_ascii=False, indent=2)
            logger.info("[Session] Cleared local session file")
    except Exception as e:
        logger.error(f"[Session] Failed to clear local file: {e}")
        local_success = False
    
    if cloud_success and local_success:
        logger.info("[Session] Cleared current session (cloud + local)")
        return {"status": "cleared"}
    else:
        return {"status": "partial", "message": f"äº‘ç«¯: {'æˆåŠŸ' if cloud_success else 'å¤±è´¥'}, æœ¬åœ°: {'æˆåŠŸ' if local_success else 'å¤±è´¥'}"}

class SummaryUpdateRequest(BaseModel):
    summary: str

@app.put("/session/summary")
async def update_summary(req: SummaryUpdateRequest):
    """æ›´æ–°æ‘˜è¦å†…å®¹ï¼ˆä¿ç•™å¯¹è¯å†å²ï¼‰"""
    try:
        existing = {"history": [], "summary": ""}
        if os.path.exists(Config.SESSION_FILE):
            with open(Config.SESSION_FILE, "r", encoding="utf-8") as f:
                existing = json.load(f)
        existing["summary"] = req.summary
        with open(Config.SESSION_FILE, "w", encoding="utf-8") as f:
            json.dump(existing, f, ensure_ascii=False, indent=2)
        logger.info(f"[Session] Summary updated, length: {len(req.summary)}")
        return {"status": "updated", "summary": req.summary}
    except Exception as e:
        logger.error(f"Failed to update summary: {e}")
        return {"status": "error", "message": str(e)}

@app.delete("/session/message/{index}")
async def delete_message(index: int):
    """åˆ é™¤æŒ‡å®šç´¢å¼•çš„å¯¹è¯æ¶ˆæ¯ï¼ˆåŒæ—¶åˆ é™¤å¯¹åº”çš„ AI å›å¤ï¼‰"""
    try:
        if not os.path.exists(Config.SESSION_FILE):
            return {"status": "error", "message": "Session file not found"}
        
        with open(Config.SESSION_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        history = data.get("history", [])
        if index < 0 or index >= len(history):
            return {"status": "error", "message": "Invalid index"}
        
        # è®¡ç®—è¦åˆ é™¤çš„æ¶ˆæ¯æ•°é‡ï¼ˆç”¨æˆ·æ¶ˆæ¯ + åç»­çš„ AI/system æ¶ˆæ¯ï¼‰
        deleted = [history[index]]
        
        # å¦‚æœåˆ é™¤çš„æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼ŒåŒæ—¶åˆ é™¤åç»­çš„ AI å›å¤å’Œå¯èƒ½çš„ system æ¶ˆæ¯
        i = index + 1
        while i < len(history) and history[i]["role"] != "user":
            deleted.append(history[i])
            i += 1
        
        # ä» history ä¸­ç§»é™¤
        for _ in range(len(deleted)):
            if index < len(history):
                history.pop(index)
        
        data["history"] = history
        with open(Config.SESSION_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"[Session] Deleted {len(deleted)} messages starting at index {index}")
        return {"status": "deleted", "count": len(deleted), "history": history}
    except Exception as e:
        logger.error(f"Failed to delete message: {e}")
        return {"status": "error", "message": str(e)}

# [REMOVED] TodoItem å’Œ todos API å·²ç§»é™¤ (V3.0 ç®€åŒ–)

# ===== è®¤çŸ¥çƒ V2.3 æ–°å¢æ¥å£ =====
from src.agents.memory_tools import fetch_memory, list_available_memories, MEMORY_TOOLS, read_memory_readonly
from src.agents.daily_archive import trigger_daily_archive as do_daily_archive

class MemoryRequest(BaseModel):
    filename: str
    keywords: str = None

@app.post("/memory/fetch")
async def api_fetch_memory(req: MemoryRequest):
    """è·å–é•¿æœŸè®°å¿† (M3) - ä¼šæ›´æ–°è®¿é—®æ—¶é—´"""
    result = await fetch_memory(req.filename, req.keywords)
    return result

@app.post("/memory/read")
async def api_read_memory(req: MemoryRequest):
    """åªè¯»è·å–è®°å¿†æ–‡ä»¶ï¼ˆä¸æ›´æ–°æ—¶é—´æˆ³ï¼ŒDebug ç”¨ï¼‰"""
    result = await read_memory_readonly(req.filename)
    return result

class DeleteMemoryRequest(BaseModel):
    filename: str

@app.delete("/memory/delete")
async def api_delete_memory(req: DeleteMemoryRequest):
    """åˆ é™¤è®°å¿†æ–‡ä»¶"""
    from src.storage.sphere_storage import get_sphere_storage
    storage = get_sphere_storage()
    success = await storage.delete_memory_file(req.filename)
    if success:
        logger.info(f"[API] è®°å¿†æ–‡ä»¶å·²åˆ é™¤: {req.filename}")
        return {"success": True, "message": f"æ–‡ä»¶ {req.filename} å·²åˆ é™¤"}
    else:
        return {"success": False, "error": f"åˆ é™¤æ–‡ä»¶ {req.filename} å¤±è´¥"}

@app.get("/memory/list")
async def api_list_memories():
    """åˆ—å‡ºå¯ç”¨çš„è®°å¿†æ–‡ä»¶"""
    files = await list_available_memories()
    return {"files": files}

@app.get("/debug/status")
async def debug_status():
    """Debug: è·å–ç³»ç»ŸçŠ¶æ€"""
    from src.utils.date_helper import get_current_logical_date, format_logical_date
    from src.storage.sphere_storage import get_sphere_storage
    
    storage = get_sphere_storage()
    session_data = await storage.load_current_session()
    
    # è·å–è®°å¿†æ–‡ä»¶æ•°é‡
    try:
        memories_response = await list_available_memories()
        memory_count = len(memories_response.get("memories", []))
    except:
        memory_count = 0
    
    return {
        "logical_date": format_logical_date(get_current_logical_date()),
        "session_count": len(session_data.get("history", [])),
        "summary_length": len(session_data.get("summary", "")),
        "memory_count": memory_count,
        "system_time": datetime.now().isoformat()
    }

@app.post("/debug/archive")
async def debug_archive():
    """Debug: æ‰‹åŠ¨è§¦å‘å½’æ¡£"""
    try:
        from src.storage.sphere_storage import get_sphere_storage
        storage = get_sphere_storage()
        session_data = await storage.load_current_session()
        
        if not session_data.get("history"):
            return {"status": "skipped", "message": "æ²¡æœ‰å¯¹è¯è®°å½•éœ€è¦å½’æ¡£"}
        
        result = await do_daily_archive(
            session_history=session_data["history"],
            current_m2=session_data.get("summary", "")
        )
        
        return {"status": "success", "message": f"å½’æ¡£å®Œæˆ: {result.get('archive_file', 'N/A')}"}
    except Exception as e:
        logger.error(f"Debugå½’æ¡£å¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/memory/tools")
async def api_get_memory_tools():
    """è·å–è®°å¿†å·¥å…·å®šä¹‰ (ä¾›å‰ç«¯ Function Calling ä½¿ç”¨)"""
    return {"tools": MEMORY_TOOLS}

class ArchiveRequest(BaseModel):
    history: list
    summary: str

@app.post("/archive/trigger")
async def api_trigger_archive(req: ArchiveRequest):
    """æ‰‹åŠ¨è§¦å‘æ¯æ—¥å½’æ¡£ä»»åŠ¡"""
    result = await do_daily_archive(req.history, req.summary)
    return result


# é™æ€æ–‡ä»¶æœåŠ¡
if os.path.exists(Config.FRONTEND_PATH):
    app.mount("/static", StaticFiles(directory=Config.FRONTEND_PATH), name="static")

@app.get("/")
async def read_index():
    """å…¥å£é‡å®šå‘ï¼šè®¿é—®æ ¹è·¯å¾„æ—¶ç›´æ¥è¿”å›èŠå¤©ç•Œé¢"""
    index_file = os.path.join(Config.FRONTEND_PATH, "index.html")
    if os.path.exists(index_file):
        return FileResponse(index_file)
    return {"message": "Frontend index.html not found. Please check 'frontend' folder."}

@app.get("/debug")
async def read_debug():
    """è°ƒè¯•é¢æ¿å…¥å£"""
    debug_file = os.path.join(Config.FRONTEND_PATH, "debug.html")
    if os.path.exists(debug_file):
        return FileResponse(debug_file)
    return {"message": "debug.html not found"}

@app.get("/debug/prompt")
async def get_debug_prompt():
    """è·å–æœ€è¿‘çš„ Prompt æ—¥å¿—"""
    try:
        with open(Config.DEBUG_PROMPT_FILE, "r", encoding="utf-8") as f:
            return {"content": f.read()}
    except FileNotFoundError:
        return {"content": "æš‚æ—  Prompt æ—¥å¿—ã€‚å…ˆè¿›è¡Œä¸€æ¬¡å¯¹è¯åå†åˆ·æ–°ã€‚"}


# è¾…åŠ©å‡½æ•°
def write_debug_prompt(messages: list) -> None:
    """å†™å…¥è°ƒè¯• Prompt åˆ°æ–‡ä»¶"""
    try:
        debug_info = f"\n{'='*50}\nTIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')}\n"
        for i, m in enumerate(messages):
            role = "SYSTEM" if isinstance(m, SystemMessage) else "USER" if isinstance(m, HumanMessage) else "ASSISTANT"
            debug_info += f"\n[{i}] {role}:\n{m.content}\n"
        debug_info += f"{'='*50}\n"
        with open(Config.DEBUG_PROMPT_FILE, "w", encoding="utf-8") as df:
            df.write(debug_info)
    except Exception as e:
        logger.error(f"Failed to write debug prompt: {e}")

def build_system_prompt(summary: str, memory_files: list) -> str:
    """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
    system_content = ""
    
    if summary:
        system_content += f"\n\nã€å‰æƒ…æè¦ï¼ˆåŠ¨æ€è®°å¿†ï¼‰ã€‘ï¼š\n{summary}"
        
    if memory_files:
        system_content += f"\n\nã€å¯ç”¨é•¿æœŸè®°å¿†æ–‡ä»¶ã€‘ï¼š{', '.join(memory_files)}\n\n**é‡è¦æé†’**ï¼š\n1. åœ¨è°ƒç”¨ fetch_memory å·¥å…·å‰ï¼Œè¯·**åŠ¡å¿…å…ˆä»”ç»†æ£€æŸ¥å¯¹è¯å†å²**ä¸­æ˜¯å¦å·²ç»åŒ…å«ç›¸å…³çš„è®°å¿†å†…å®¹\n2. å¦‚æœå†å²ä¸­æœ‰ [å·²æ£€ç´¢çš„é•¿æœŸè®°å¿†] æ ‡è®°çš„å†…å®¹ï¼Œè¯´æ˜ç›¸å…³è®°å¿†å·²ç»è·å–è¿‡ï¼Œ**ä¸è¦é‡å¤è°ƒç”¨å·¥å…·**\n3. åªæœ‰å½“å†å²ä¸­ç¡®å®æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œæ‰è°ƒç”¨ fetch_memory å·¥å…·\n4. ä¼˜å…ˆä½¿ç”¨å†å²ä¸­å·²æœ‰çš„è®°å¿†å†…å®¹æ¥å›ç­”é—®é¢˜"
    
    # è°ƒè¯•ï¼šè¾“å‡ºç³»ç»Ÿæç¤ºè¯
    logger.info(f"[DEBUG] System prompt built: {system_content[:200]}...")
    return system_content

def build_messages(system_content: str, history: list, current_message: str) -> list:
    """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
    messages = [SystemMessage(content=system_content)]
    for h in history:
        if h["role"] == "user":
            messages.append(HumanMessage(content=h["content"]))
        else:
            messages.append(AIMessage(content=h["content"]))
    messages.append(HumanMessage(content=current_message))
    return messages

def save_session_if_needed(auto_save: bool, history: list, summary: str) -> None:
    """æ ¹æ®éœ€è¦ä¿å­˜ä¼šè¯"""
    if auto_save:
        os.makedirs("data", exist_ok=True)
        with open(Config.SESSION_FILE, "w", encoding="utf-8") as f:
            json.dump({"history": history, "summary": summary}, f, ensure_ascii=False, indent=2)
        logger.info(f"[Session] Auto-saved to file, history length: {len(history)}")
    else:
        logger.info("[Session] auto_save=False, skipped saving")


@app.post("/chat")
async def chat_with_agent(req: ChatRequest):
    """
    ä¸‰å±‚è®°å¿†æ¶æ„å¯¹è¯æ¥å£ (TMA Stage 1) - æµå¼ç‰ˆæœ¬:
    1. åŠ¨æ€æ³¨å…¥ L2 æ‘˜è¦ä½œä¸ºâ€œé•¿æœŸèƒŒæ™¯â€
    2. ä½¿ç”¨ StreamingResponse å®ç°æ‰“å­—æœºæ•ˆæœ
    3. åœ¨æµç»“æŸæ—¶å›ä¼  metadata (summary & history)
    """
    import sys, time
    from datetime import datetime
    start_time = time.time()
    # å¼ºåˆ¶åœ¨å‡½æ•°å…¥å£æ‰“æ¡©ï¼Œä¸ä¾èµ– generator å¼€å§‹æ‰§è¡Œ
    entry_msg = f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ [BACKEND HIT] /chat endpoint reached.\n"
    sys.stderr.write(entry_msg)
    sys.stderr.flush()
    
    logger.info("--- [Stream Chat Session Start] ---")
    
    async def chat_generator():
        banner = f"\n{'='*30}\nğŸŸ¢ NEW STREAMING REQUEST AT {datetime.now().strftime('%H:%M:%S.%f')[:-3]}\n{'='*30}\n"
        sys.stderr.write(banner)
        sys.stderr.write(f"RAW USER TEXT: {req.message}\n")
        sys.stderr.flush()
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯å’Œæ¶ˆæ¯
        from src.agents.memory_tools import list_available_memories
        memory_files = await list_available_memories()
        system_content = build_system_prompt(req.summary, memory_files)
        messages = build_messages(system_content, req.history, req.message)
        
        # æ—¥å¿—è¿½è¸ª
        logger.info(f">>> [System Prompt Context]:\n{system_content}")
        logger.info(f">>> [Chat History Window]: {len(req.history)} messages")

        # å†™å…¥è°ƒè¯•æ—¥å¿—
        write_debug_prompt(messages)
        sys.stderr.write(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ“ Raw prompt synced to debug file\n")
        sys.stderr.flush()
        
        full_content = ""
        m3_context = ""  # å­˜å‚¨æ£€ç´¢åˆ°çš„é•¿æœŸè®°å¿†
        use_thinking_mode = True  # å¿…é¡»ä½¿ç”¨thinking mode
        
        # --- å®šä¹‰å·¥å…· ---
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "fetch_memory",
                    "description": "æ£€ç´¢é•¿æœŸè®°å¿†ã€‚å½“ç”¨æˆ·è¯¢é—®å†å²äº‹ä»¶ã€ä¸ªäººåå¥½ã€è¿‡å¾€å†³ç­–ã€èŒä¸šè§„åˆ’ã€è´¢åŠ¡èµ„äº§ç­‰éœ€è¦æŸ¥é˜…è®°å¿†åº“çš„å†…å®¹æ—¶è°ƒç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "filename": {"type": "string", "description": "è®°å¿†æ–‡ä»¶åï¼Œå¦‚ï¼šèŒä¸šè§„åˆ’.md, è´¢åŠ¡èµ„äº§.md, å¥åº·ç®¡ç†.md, æƒ…æ„Ÿè®°å½•.md"},
                            "keywords": {"type": "string", "description": "å¯é€‰æœç´¢å…³é”®è¯ï¼Œç”¨äºç²¾ç¡®åŒ¹é…æ®µè½"}
                        },
                        "required": ["filename"]
                    }
                }
            }
        ] if memory_files else []
        
        # è°ƒè¯•ï¼šè¾“å‡ºå·¥å…·å®šä¹‰
        logger.info(f"[Tools Debug] å¯ç”¨å·¥å…·æ•°é‡: {len(tools)}")
        logger.info(f"[Tools Debug] è®°å¿†æ–‡ä»¶æ•°é‡: {len(memory_files) if memory_files else 0}")
        if tools:
            logger.info(f"[Tools Debug] å·¥å…·å®šä¹‰: {json.dumps(tools[0], ensure_ascii=False, indent=2)}")
        
        # --- å·¥å…·æ‰§è¡Œå™¨ ---
        async def execute_tool(name: str, args: dict) -> str:
            """æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ"""
            nonlocal m3_context, system_content
            if name == "fetch_memory":
                filename = args.get("filename", "")
                keywords = args.get("keywords")
                sys.stderr.write(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”§ Executing fetch_memory({filename})\n")
                sys.stderr.flush()
                result = await fetch_memory(filename, keywords)
                if result["success"]:
                    content = result["content"]
                    m3_context += f"\n\nã€æ¥è‡ª {filename} çš„é•¿æœŸè®°å¿†ã€‘ï¼š\n{content}"
                    logger.info(f"[M3 Success] è·å–åˆ° {len(content)} å­—ç¬¦")
                    return content
                else:
                    return f"æœªæ‰¾åˆ°æ–‡ä»¶: {filename}"
            return f"æœªçŸ¥å·¥å…·: {name}"
        
        try:
            # --- Thinking Mode + Tool Calls (V3.2 æ–°ç‰¹æ€§) ---
            if use_thinking_mode and tools:
                yield "event: status\ndata: ğŸ’­ æ­£åœ¨æ€è€ƒå¹¶æŸ¥é˜…è®°å¿†...\n\n"
                sys.stderr.write(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ§  Using Thinking Mode + Tool Calls\n")
                sys.stderr.flush()
                
                from src.agents.thinking_tool_stream import stream_with_thinking_tools, ChunkType
                
                # è½¬æ¢ LangChain messages ä¸º OpenAI æ ¼å¼
                openai_messages = []
                for m in messages:
                    if isinstance(m, SystemMessage):
                        openai_messages.append({"role": "system", "content": m.content})
                    elif isinstance(m, HumanMessage):
                        openai_messages.append({"role": "user", "content": m.content})
                    elif isinstance(m, AIMessage):
                        openai_messages.append({"role": "assistant", "content": m.content})
                
                thinking_start = time.time()
                try:
                    async for chunk in stream_with_thinking_tools(
                        messages=openai_messages,
                        tools=tools,
                        tool_executor=execute_tool,
                        max_tool_rounds=10  # å¢åŠ åˆ°10è½®ï¼Œæ”¯æŒè¯»å–æ‰€æœ‰è®°å¿†æ–‡ä»¶
                    ):
                        if chunk.type == ChunkType.TOOL_CALL:
                            # æ˜¾ç¤ºå…·ä½“çš„å·¥å…·å‚æ•°ï¼Œè®©ç”¨æˆ·çŸ¥é“åœ¨æŸ¥é˜…å“ªä¸ªæ–‡ä»¶
                            tool_info = chunk.tool_call or {}
                            tool_name = tool_info.get("name", "unknown")
                            tool_args = tool_info.get("args", {})
                            if tool_name == "fetch_memory":
                                filename = tool_args.get("filename", "")
                                yield f"event: status\ndata: ğŸ“‚ æ­£åœ¨æŸ¥é˜…è®°å¿†ï¼š{filename}\n\n"
                            else:
                                yield f"event: status\ndata: ğŸ”§ {chunk.content}\n\n"
                        elif chunk.type == ChunkType.CONTENT:
                            full_content += chunk.content
                            # ä¿®å¤æ¢è¡Œç¬¦é—®é¢˜ï¼šå°†å†…å®¹ä¸­çš„æ¢è¡Œç¬¦è½¬æ¢ä¸ºSSEæ ¼å¼
                            content_lines = chunk.content.split('\n')
                            if len(content_lines) == 1:
                                # å•è¡Œå†…å®¹
                                yield f"event: content\ndata: {chunk.content}\n\n"
                            else:
                                # å¤šè¡Œå†…å®¹ï¼Œæ¯è¡Œéƒ½è¦åŠ data:å‰ç¼€
                                sse_content = "event: content\n"
                                for line in content_lines:
                                    sse_content += f"data: {line}\n"
                                sse_content += "\n"
                                yield sse_content
                        elif chunk.type == ChunkType.ERROR:
                            # Thinking Mode å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
                            logger.warning(f"[Thinking Mode] Error: {chunk.content}, falling back...")
                            sys.stderr.write(f"[{datetime.now().strftime('%H:%M:%S')}] âš ï¸ Thinking Mode failed, fallback\n")
                            use_thinking_mode = False
                            break
                    else:
                        # æ­£å¸¸å®Œæˆ
                        thinking_time = time.time() - thinking_start
                        sys.stderr.write(f"[{datetime.now().strftime('%H:%M:%S')}] âœ… Thinking Mode done ({thinking_time:.2f}s)\n")
                        logger.info(f"[Thinking Mode] å®Œæˆï¼Œè€—æ—¶ {thinking_time:.2f}s")
                        
                except Exception as e:
                    logger.error(f"[Thinking Mode] Exception: {e}")
                    sys.stderr.write(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ Thinking Mode exception: {e}\n")
                    use_thinking_mode = False
            
            # --- Fallback: æ™®é€šæµå¼è°ƒç”¨ (ä¸ä½¿ç”¨ Thinking Mode) ---
            if not use_thinking_mode or not full_content:
                if not full_content:  # åªæœ‰åœ¨æ²¡æœ‰ç”Ÿæˆå†…å®¹æ—¶æ‰å›é€€
                    yield "event: status\ndata: âœ¨ æ­£åœ¨ç”Ÿæˆå›å¤...\n\n"
                    sys.stderr.write(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“ Fallback to standard streaming\n")
                    
                    # å¦‚æœå·²ç»è·å–äº†è®°å¿†å†…å®¹ï¼Œæ³¨å…¥åˆ° system prompt
                    if m3_context:
                        system_content += m3_context
                        messages[0] = SystemMessage(content=system_content)
                    
                    async for chunk in llm.astream(messages):
                        token = chunk.content
                        full_content += token
                        yield f"event: content\ndata: {token}\n\n"
            
            chat_done_time = time.time()
            logger.info(f"LLM First Response Latency: {chat_done_time - start_time:.2f}s")

            # 2. å¯¹è¯ç»“æŸåï¼Œå¤„ç†è®°å¿†é€»è¾‘ (L2 å‹ç¼©)
            new_summary = req.summary
            
            # æ„å»ºæ–°å†å²ï¼šå¦‚æœæœ¬æ¬¡è·å–äº†è®°å¿†ï¼ŒæŠŠè®°å¿†å†…å®¹ä¹ŸåŠ å…¥å†å²
            # è¿™æ ·åç»­å¯¹è¯æ¨¡å‹å°±çŸ¥é“å·²ç»è¯»å–è¿‡å“ªäº›è®°å¿†ï¼Œé¿å…é‡å¤è°ƒç”¨å·¥å…·
            new_history = req.history.copy()
            new_history.append({"role": "user", "content": req.message})
            
            # å¦‚æœæœ‰è®°å¿†å†…å®¹ï¼Œä½œä¸ºç³»ç»Ÿæ¶ˆæ¯æ³¨å…¥å†å²ï¼ˆç”¨æˆ·ä¸å¯è§ï¼Œä½†æ¨¡å‹å¯è§ï¼‰
            if m3_context:
                new_history.append({
                    "role": "system", 
                    "content": f"[å·²æ£€ç´¢çš„é•¿æœŸè®°å¿†]{m3_context}"
                })
                logger.info(f"[Memory Injected] å·²å°† {len(m3_context)} å­—ç¬¦çš„è®°å¿†å†…å®¹æ³¨å…¥å†å²")
            
            # æ¸…ç†å¯èƒ½æ··å…¥çš„ [STATUS] æ ‡è®°
            clean_content = full_content
            import re
            clean_content = re.sub(r'\[STATUS\][^\n]*\n?', '', clean_content).strip()
            
            new_history.append({"role": "ai", "content": clean_content})
            
            # æ³¨ï¼šæ‘˜è¦å‹ç¼©é€»è¾‘å·²ç§»é™¤
            # æ‘˜è¦åªåœ¨å‡Œæ™¨è‡ªåŠ¨ä»»åŠ¡æˆ–æ‰‹åŠ¨å½’æ¡£æ—¶æ›´æ–°ï¼Œä¸åœ¨æ¯æ¬¡å¯¹è¯æ—¶è§¦å‘
            # å‚è§ daily_archive.py çš„ trigger_daily_archive() å‡½æ•°

            # [V3.0] world_state_prompt å·²ç§»é™¤ï¼Œä¸å†è‡ªåŠ¨æå– pinned_facts/todos

            # 3. å‘é€å…ƒæ•°æ®æ ‡è®°ä½
            try:
                end_time = time.time()
                metadata = {
                    "type": "metadata",
                    "summary": new_summary,
                    "history": new_history,

                    "debug": {
                        "raw_prompt": [
                            {"role": "system" if isinstance(m, SystemMessage) else "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content} 
                            for m in messages
                        ],
                        "latency": {
                            "llm_chat": f"{chat_done_time - start_time:.2f}s",
                            "total": f"{end_time - start_time:.2f}s"
                        },
                        "system_prompt": system_content,
                        "history_count": len(req.history)
                    }
                }
                meta_json = json.dumps(metadata, ensure_ascii=False)
                yield f"event: metadata\ndata: {meta_json}\n\n"
                
                # æ ¹æ® auto_save å‚æ•°å†³å®šæ˜¯å¦è‡ªåŠ¨ä¿å­˜
                if req.auto_save:
                    # ä¸»è¦ä¿å­˜åˆ°äº‘ç«¯
                    from src.storage.sphere_storage import get_sphere_storage
                    storage = get_sphere_storage()
                    await storage.save_current_session(new_history, new_summary)
                    logger.info(f"[Session] Auto-saved to cloud, history length: {len(new_history)}")
                else:
                    logger.info(f"[Session] auto_save=False, skipped saving")
                
                logger.info(f"--- [Stream Chat End] Total Latency: {end_time - start_time:.2f}s ---")
                # ä¸åœ¨è¿™é‡Œå‘é€doneäº‹ä»¶ï¼Œç»Ÿä¸€åœ¨finallyä¸­å‘é€
            except Exception as me:
                logger.error(f"Metadata generation failed: {me}")
                yield f"event: error\ndata: {{\"error\": \"metadata_failed\"}}\n\n"
                # ä¸åœ¨è¿™é‡Œå‘é€doneäº‹ä»¶ï¼Œç»Ÿä¸€åœ¨finallyä¸­å‘é€

        except Exception as e:
            logger.error(f"Streaming failed: {e}", exc_info=True)
            yield f"event: error\ndata: {{\"error\": \"streaming_failed\", \"message\": \"{str(e)}\"}}\n\n"
        finally:
            yield "event: done\ndata: {}\n\n"

    return StreamingResponse(chat_generator(), media_type="text/event-stream")

if __name__ == "__main__":
    # å¯åŠ¨ Uvicornï¼Œä¼˜å…ˆè¯»å– HF ç¯å¢ƒè¦æ±‚çš„ç«¯å£
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"Sphere Backend Server is launching on port {port}...")
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)
